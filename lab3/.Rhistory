### FORWARD/ALPHA/FILTERED ###
forwardAlpha = exp(forward(hmm, obs))
filteredCalc <- paste(rep("z =",10), max.col(t(prop.table(forwardAlpha,2)))-1)
filteredAcc <- sum(filteredCalc == sim$states)/length(filteredCalc)
filteredAcc
### SMOOTHED (manually) ###
backwardBeta <- exp(backward(hmm, obs))
smoothingCalcMan <- paste(rep("z =",10), max.col(t(prop.table(forwardAlpha*backwardBeta,2)))-1)
smoothingAccMan <- sum(smoothingCalcMan == sim$states)/length(smoothingCalcMan)
smoothingAccMan
### SMOOTHED (using posterior) ###
smoothingCalc <- paste(rep("z =", 10),max.col(t(posterior(hmm, obs)))-1)
smoothingAcc <- sum(smoothingCalc == sim$states)/length(smoothingCalc)
smoothingAcc
### PATH VITERBI ###
pathCalc <- viterbi(hmm, obs)
viterbiAcc <- sum(pathCalc == sim$states)/length(pathCalc)
viterbiAcc
### Check last index, step 100 ###
prop.table(forwardAlpha*backwardBeta,2)[,100] == prop.table(forwardAlpha,2)[,100] # True for all states
### Question 1: Smoothed is using more information: the whole set of observed emission x[0:T] whereas filtered only uses data emitted up to that point x[0:t]
### Question 2: The viterbi algoritm generates a most likely path, i.e. it has to abide to the constraints of the transitions between states in the hidden variables
### The smooth approximation approximate the most likely state, and can therefor make "illegal" moves from one state to another when predicting current state.
### Task 6 ###
entropyFiltering <- apply(prop.table(forwardAlpha, 2), 2, entropy.empirical)
entropySmoothing <- apply(prop.table(forwardAlpha*backwardBeta,2), 2, entropy.empirical)
sum(entropyFiltering < entropySmoothing)
matplot(1:100, entropyFiltering, type = "l", col = "red", xlab = "Step", ylab = "Entropy")
lines(entropySmoothing,type="l",col="blue")
legend(10, 1.6, legend=c("Filtering", "Smoothing"),
col=c("red", "blue"), lty=1:1)
filteredAccFirstHalf <- sum(filteredCalc[1:50] == sim$states[1:50])/length(filteredCalc[1:50])
filteredAccFirstHalf
filteredAccSecondHalf <- sum(filteredCalc[51:100] == sim$states[51:100])/length(filteredCalc[51:100])
filteredAccSecondHalf
### Task 7 ###
step100 <- prop.table(forwardAlpha,2)[,100]
step101 <- 0.5*step100 + 0.5*c(step100[10], step100[1:9])
install.packages("data.table")
library(HMM)
library(entropy)
library(data.table)
### TASK ONE ###
states <- paste(rep("z =",10), 0:9)
stateNames <- paste(rep("z =",10), 0:9)
stepProbs <- cbind(c(numeric(9),1/2),1/2*diag(x=10))
stepProbs <- stepProbs[,-11] + 1/2*diag(10)
dimnames(stepProbs) <-list(as.character(0:9), as.character(0:9))
emissionProbs <- c(1/5, 1/5, 1/5, 0, 0, 0, 0, 0, 1/5, 1/5)
for (i in 1:10) {
emissionProbs <- c(emissionProbs, shift(tail(emissionProbs,n=10), n=1, fill = tail(emissionProbs,n=1)))
}
emissionProbs <- matrix(emissionProbs, 10, 10, byrow=TRUE, dimnames=list(stateNames, paste(rep("p(x=",10),as.character(0:9),rep(")",10))))
#emissionProbs <- matrix(c(1/5, 1/5, 1/5, 0, 0, 0, 0, 0, 1/5, 1/5,
#                          1/5, 1/5, 1/5, 1/5, 0, 0, 0, 0, 0, 1/5,
#                          1/5, 1/5, 1/5, 1/5, 1/5, 0, 0, 0, 0, 0,
#                          0, 1/5, 1/5, 1/5, 1/5, 1/5, 0, 0, 0, 0,
#                          0, 0, 1/5, 1/5, 1/5, 1/5, 1/5, 0, 0, 0,
#                          0, 0, 0, 1/5, 1/5, 1/5, 1/5, 1/5, 0, 0,
#                          0, 0, 0, 0, 1/5, 1/5, 1/5, 1/5, 1/5, 0,
#                          0, 0, 0, 0, 0, 1/5, 1/5, 1/5, 1/5, 1/5,
#                          1/5, 0, 0, 0, 0, 0, 1/5, 1/5, 1/5, 1/5,
#                          1/5, 1/5, 0, 0, 0, 0, 0, 1/5, 1/5, 1/5),
#                        10, 10, byrow=TRUE,
#                        dimnames=list(stateNames, paste(rep("p(x=",10),as.character(0:9),rep(")",10))))
hmm = initHMM(stateNames, states, transProbs = stepProbs, emissionProbs = emissionProbs)
### TASK TWO ###
nSim <- 100
sim <- simHMM(hmm,nSim)
obs <- sim$observation
trueStates <- sim$states
### TASK THREE, FOUR AND FIVE ###
### FORWARD/ALPHA/FILTERED ###
forwardAlpha = exp(forward(hmm, obs))
filteredCalc <- paste(rep("z =",10), max.col(t(prop.table(forwardAlpha,2)))-1)
filteredAcc <- sum(filteredCalc == sim$states)/length(filteredCalc)
filteredAcc
### SMOOTHED (manually) ###
backwardBeta <- exp(backward(hmm, obs))
smoothingCalcMan <- paste(rep("z =",10), max.col(t(prop.table(forwardAlpha*backwardBeta,2)))-1)
smoothingAccMan <- sum(smoothingCalcMan == sim$states)/length(smoothingCalcMan)
smoothingAccMan
### SMOOTHED (using posterior) ###
smoothingCalc <- paste(rep("z =", 10),max.col(t(posterior(hmm, obs)))-1)
smoothingAcc <- sum(smoothingCalc == sim$states)/length(smoothingCalc)
smoothingAcc
### PATH VITERBI ###
pathCalc <- viterbi(hmm, obs)
viterbiAcc <- sum(pathCalc == sim$states)/length(pathCalc)
viterbiAcc
### Check last index, step 100 ###
prop.table(forwardAlpha*backwardBeta,2)[,100] == prop.table(forwardAlpha,2)[,100] # True for all states
### Question 1: Smoothed is using more information: the whole set of observed emission x[0:T] whereas filtered only uses data emitted up to that point x[0:t]
### Question 2: The viterbi algoritm generates a most likely path, i.e. it has to abide to the constraints of the transitions between states in the hidden variables
### The smooth approximation approximate the most likely state, and can therefor make "illegal" moves from one state to another when predicting current state.
### Task 6 ###
entropyFiltering <- apply(prop.table(forwardAlpha, 2), 2, entropy.empirical)
entropySmoothing <- apply(prop.table(forwardAlpha*backwardBeta,2), 2, entropy.empirical)
sum(entropyFiltering < entropySmoothing)
matplot(1:100, entropyFiltering, type = "l", col = "red", xlab = "Step", ylab = "Entropy")
lines(entropySmoothing,type="l",col="blue")
legend(10, 1.6, legend=c("Filtering", "Smoothing"),
col=c("red", "blue"), lty=1:1)
filteredAccFirstHalf <- sum(filteredCalc[1:50] == sim$states[1:50])/length(filteredCalc[1:50])
filteredAccFirstHalf
filteredAccSecondHalf <- sum(filteredCalc[51:100] == sim$states[51:100])/length(filteredCalc[51:100])
filteredAccSecondHalf
### Task 7 ###
step100 <- prop.table(forwardAlpha,2)[,100]
step101 <- 0.5*step100 + 0.5*c(step100[10], step100[1:9])
library(HMM)
library(entropy)
library(data.table)
### TASK ONE ###
states <- paste(rep("z =",10), 0:9)
stateNames <- paste(rep("z =",10), 0:9)
stepProbs <- cbind(c(numeric(9),1/2),1/2*diag(x=10))
stepProbs <- stepProbs[,-11] + 1/2*diag(10)
dimnames(stepProbs) <-list(as.character(0:9), as.character(0:9))
emissionProbs <- c(1/5, 1/5, 1/5, 0, 0, 0, 0, 0, 1/5, 1/5)
for (i in 1:10) {
emissionProbs <- c(emissionProbs, shift(tail(emissionProbs,n=10), n=1, fill = tail(emissionProbs,n=1)))
}
emissionProbs <- matrix(emissionProbs, 10, 10, byrow=TRUE, dimnames=list(stateNames, paste(rep("p(x=",10),as.character(0:9),rep(")",10))))
hmm = initHMM(stateNames, states, transProbs = stepProbs, emissionProbs = emissionProbs)
### TASK TWO ###
nSim <- 100
sim <- simHMM(hmm,nSim)
obs <- sim$observation
trueStates <- sim$states
### TASK THREE AND FOUR###
### FORWARD/ALPHA/FILTERED ###
forwardAlpha = exp(forward(hmm, obs))
filteredCalc <- paste(rep("z =",10), max.col(t(prop.table(forwardAlpha,2)))-1)
filteredAcc <- sum(filteredCalc == sim$states)/length(filteredCalc)
filteredAcc
### SMOOTHED (manually) ###
backwardBeta <- exp(backward(hmm, obs))
smoothingCalcMan <- paste(rep("z =",10), max.col(t(prop.table(forwardAlpha*backwardBeta,2)))-1)
smoothingAccMan <- sum(smoothingCalcMan == sim$states)/length(smoothingCalcMan)
smoothingAccMan
### SMOOTHED (using posterior) ###
smoothingCalc <- paste(rep("z =", 10),max.col(t(posterior(hmm, obs)))-1)
smoothingAcc <- sum(smoothingCalc == sim$states)/length(smoothingCalc)
smoothingAcc
### PATH VITERBI ###
pathCalc <- viterbi(hmm, obs)
viterbiAcc <- sum(pathCalc == sim$states)/length(pathCalc)
viterbiAcc
### TASK FIVE ###
### Question 1: Smoothed is using more information: the whole set of observed emission x[0:T] whereas filtered only uses data emitted up to that point x[0:t]
### Question 2: The viterbi algoritm generates a most likely path, i.e. it has to abide to the constraints of the transitions between states in the hidden variables
### The smooth approximation approximate the most likely state, and can therefor make "illegal" moves from one state to another when predicting current state.
### TASK SIX ###
entropyFiltering <- apply(prop.table(forwardAlpha, 2), 2, entropy.empirical)
entropySmoothing <- apply(prop.table(forwardAlpha*backwardBeta,2), 2, entropy.empirical)
sum(entropyFiltering < entropySmoothing)
matplot(1:100, entropyFiltering, type = "l", col = "red", xlab = "Step", ylab = "Entropy")
lines(entropySmoothing,type="l",col="blue")
legend(10, 1.6, legend=c("Filtering", "Smoothing"),
col=c("red", "blue"), lty=1:1)
filteredAccFirstHalf <- sum(filteredCalc[1:50] == sim$states[1:50])/length(filteredCalc[1:50])
filteredAccFirstHalf
filteredAccSecondHalf <- sum(filteredCalc[51:100] == sim$states[51:100])/length(filteredCalc[51:100])
filteredAccSecondHalf
### TASK SEVEN ###
### Check last index, step 100 ###
prop.table(forwardAlpha*backwardBeta,2)[,100] == prop.table(forwardAlpha,2)[,100] # True for all states
step100 <- prop.table(forwardAlpha,2)[,100]
step101 <- 0.5*step100 + 0.5*c(step100[10], step100[1:9])
step100
library(HMM)
library(entropy)
library(data.table)
### TASK ONE ###
states <- paste(rep("z =",10), 0:9)
stateNames <- paste(rep("z =",10), 0:9)
stepProbs <- cbind(c(numeric(9),1/2),1/2*diag(x=10))
stepProbs <- stepProbs[,-11] + 1/2*diag(10)
dimnames(stepProbs) <-list(as.character(0:9), as.character(0:9))
emissionProbs <- c(1/5, 1/5, 1/5, 0, 0, 0, 0, 0, 1/5, 1/5)
for (i in 1:10) {
emissionProbs <- c(emissionProbs, shift(tail(emissionProbs,n=10), n=1, fill = tail(emissionProbs,n=1)))
}
emissionProbs <- matrix(emissionProbs, 10, 10, byrow=TRUE, dimnames=list(stateNames, paste(rep("p(x=",10),as.character(0:9),rep(")",10))))
hmm = initHMM(stateNames, states, transProbs = stepProbs, emissionProbs = emissionProbs)
### TASK TWO ###
nSim <- 100
sim <- simHMM(hmm,nSim)
obs <- sim$observation
trueStates <- sim$states
### TASK THREE AND FOUR ###
### FILTERED ###
forwardAlpha = exp(forward(hmm, obs))
filteredCalc <- paste(rep("z =",10), max.col(t(prop.table(forwardAlpha,2)))-1)
filteredAcc <- sum(filteredCalc == sim$states)/length(filteredCalc)
filteredAcc
### SMOOTHED (manually) ###
backwardBeta <- exp(backward(hmm, obs))
smoothingCalcMan <- paste(rep("z =",10), max.col(t(prop.table(forwardAlpha*backwardBeta,2)))-1)
smoothingAccMan <- sum(smoothingCalcMan == sim$states)/length(smoothingCalcMan)
smoothingAccMan
### SMOOTHED (using posterior) ###
smoothingCalc <- paste(rep("z =", 10),max.col(t(posterior(hmm, obs)))-1)
smoothingAcc <- sum(smoothingCalc == sim$states)/length(smoothingCalc)
smoothingAcc
### PATH VITERBI ###
pathCalc <- viterbi(hmm, obs)
viterbiAcc <- sum(pathCalc == sim$states)/length(pathCalc)
viterbiAcc
### TASK FIVE ###
### Question 1: Smoothed is using more information: the whole set of observed emission x[0:T] whereas filtered only uses data emitted up to that point x[0:t]
### Question 2: The viterbi algoritm generates a most likely path, i.e. it has to abide to the constraints of the transitions between states in the hidden variables
### The smooth approximation approximate the most likely state, and can therefor make "illegal" moves from one state to another when predicting current state.
### TASK SIX ###
entropyFiltering <- apply(prop.table(forwardAlpha, 2), 2, entropy.empirical)
entropySmoothing <- apply(prop.table(forwardAlpha*backwardBeta,2), 2, entropy.empirical)
sum(entropyFiltering < entropySmoothing)
matplot(1:100, entropyFiltering, type = "l", col = "red", xlab = "Step", ylab = "Entropy")
lines(entropySmoothing,type="l",col="blue")
legend(10, 1.6, legend=c("Filtering", "Smoothing"),
col=c("red", "blue"), lty=1:1)
filteredAccFirstHalf <- sum(filteredCalc[1:50] == sim$states[1:50])/length(filteredCalc[1:50])
filteredAccFirstHalf
filteredAccSecondHalf <- sum(filteredCalc[51:100] == sim$states[51:100])/length(filteredCalc[51:100])
filteredAccSecondHalf # Not necessarily higher
### TASK SEVEN ###
### Check last index, step 100 ###
prop.table(forwardAlpha*backwardBeta,2)[,100] == prop.table(forwardAlpha,2)[,100] # True for all states
step100 <- prop.table(forwardAlpha,2)[,100]
step101 <- 0.5*step100 + 0.5*c(step100[10], step100[1:9])
step100
step101
specify_decimal(step100, 2)
?format
format(round(step100,4), nsmall=4)
### Check last index, step 100 ###
prop.table(forwardAlpha*backwardBeta,2)[,100] == prop.table(forwardAlpha,2)[,100] # True for all states
step100 <- prop.table(forwardAlpha,2)[,100]
step101 <- 0.5*step100 + 0.5*c(step100[10], step100[1:9])
format(round(step100,4), nsmall=4)
format(round(step101,4), nsmall=4)
states <- paste(rep("z =",10), 0:9)
stateNames <- paste(rep("z =",10), 0:9)
stepProbs <- cbind(c(numeric(9),1/2),1/2*diag(x=10))
stepProbs <- stepProbs[,-11] + 1/2*diag(10)
dimnames(stepProbs) <-list(as.character(0:9), as.character(0:9))
emissionProbs <- c(1/5, 1/5, 1/5, 0, 0, 0, 0, 0, 1/5, 1/5)
for (i in 1:10) {
emissionProbs <- c(emissionProbs, shift(tail(emissionProbs,n=10), n=1, fill = tail(emissionProbs,n=1)))
}
emissionProbs <- matrix(emissionProbs, 10, 10, byrow=TRUE, dimnames=list(stateNames, paste(rep("p(x=",10),as.character(0:9),rep(")",10))))
library(HMM)
library(entropy)
library(data.table)
### TASK ONE ###
states <- paste(rep("z =",10), 0:9)
stateNames <- paste(rep("z =",10), 0:9)
stepProbs <- cbind(c(numeric(9),1/2),1/2*diag(x=10))
stepProbs <- stepProbs[,-11] + 1/2*diag(10)
dimnames(stepProbs) <-list(as.character(0:9), as.character(0:9))
emissionProbs <- c(1/5, 1/5, 1/5, 0, 0, 0, 0, 0, 1/5, 1/5)
for (i in 1:10) {
emissionProbs <- c(emissionProbs, shift(tail(emissionProbs,n=10), n=1, fill = tail(emissionProbs,n=1)))
}
emissionProbs <- matrix(emissionProbs, 10, 10, byrow=TRUE, dimnames=list(stateNames, paste(rep("p(x=",10),as.character(0:9),rep(")",10))))
hmm = initHMM(stateNames, states, transProbs = stepProbs, emissionProbs = emissionProbs)
### TASK TWO ###
nSim <- 100
sim <- simHMM(hmm,nSim)
obs <- sim$observation
trueStates <- sim$states
### TASK THREE AND FOUR ###
### FILTERED ###
forwardAlpha = exp(forward(hmm, obs))
filteredCalc <- paste(rep("z =",10), max.col(t(prop.table(forwardAlpha,2)))-1)
filteredAcc <- sum(filteredCalc == sim$states)/length(filteredCalc)
filteredAcc
### SMOOTHED (manually) ###
backwardBeta <- exp(backward(hmm, obs))
smoothingCalcMan <- paste(rep("z =",10), max.col(t(prop.table(forwardAlpha*backwardBeta,2)))-1)
smoothingAccMan <- sum(smoothingCalcMan == sim$states)/length(smoothingCalcMan)
smoothingAccMan
### SMOOTHED (using posterior) ###
smoothingCalc <- paste(rep("z =", 10),max.col(t(posterior(hmm, obs)))-1)
smoothingAcc <- sum(smoothingCalc == sim$states)/length(smoothingCalc)
smoothingAcc
### PATH VITERBI ###
pathCalc <- viterbi(hmm, obs)
viterbiAcc <- sum(pathCalc == sim$states)/length(pathCalc)
viterbiAcc
### TASK FIVE ###
### Question 1: Smoothed is using more information: the whole set of observed emission x[0:T] whereas filtered only uses data emitted up to that point x[0:t]
### Question 2: The viterbi algoritm generates a most likely path, i.e. it has to abide to the constraints of the transitions between states in the hidden variables
### The smooth approximation approximate the most likely state, and can therefor make "illegal" moves from one state to another when predicting current state.
### TASK SIX ###
entropyFiltering <- apply(prop.table(forwardAlpha, 2), 2, entropy.empirical)
entropySmoothing <- apply(prop.table(forwardAlpha*backwardBeta,2), 2, entropy.empirical)
sum(entropyFiltering < entropySmoothing)
matplot(1:100, entropyFiltering, type = "l", col = "red", xlab = "Step", ylab = "Entropy")
lines(entropySmoothing,type="l",col="blue")
legend(10, 1.6, legend=c("Filtering", "Smoothing"),
col=c("red", "blue"), lty=1:1)
filteredAccFirstHalf <- sum(filteredCalc[1:50] == sim$states[1:50])/length(filteredCalc[1:50])
filteredAccFirstHalf
filteredAccSecondHalf <- sum(filteredCalc[51:100] == sim$states[51:100])/length(filteredCalc[51:100])
filteredAccSecondHalf # Not necessarily higher
### TASK SEVEN ###
### Check last index, step 100 ###
prop.table(forwardAlpha*backwardBeta,2)[,100] == prop.table(forwardAlpha,2)[,100] # True for all states
step100 <- prop.table(forwardAlpha,2)[,100]
step101 <- 0.5*step100 + 0.5*c(step100[10], step100[1:9])
format(round(step100,4), nsmall=4)
format(round(step101,4), nsmall=4)
pathCalc
stepProbs <- cbind(c(numeric(9),1/2),1/2*diag(x=10))
stepProbs
stepProbs <- stepProbs[,-11] + 1/2*diag(10)
stepProbs
SquaredExpKernel <- function(x1,x2,sigmaF=1,l=3){ #Implementera egen?
n1 <- length(x1)
n2 <- length(x2)
K <- matrix(NA,n1,n2)
for (i in 1:n2){
K[,i] <- sigmaF^2*exp(-0.5*( (x1-x2[i])/l)^2 )
}
return(K)
}
sigmaF <- 1
l <- 0.3
sigmaNoise <- 0.1
x <- 0.4
y <- 0.719
K <-SquaredExpKernel(x, x, sigmaF, l)
L <- t(chol(K + sigmaNoise*diag(1, nrow(K),ncol(K))))
alpha <- solve(t(L), solve(L, y))
kStar <- SquaredExpKernel(x, xStar, sigmaF, l)
MeanPosterior <- t(kStar)%*%alpha # fstar normalisera?
v <- solve(L, kStar)
VarPosterior <- SquaredExpKernel(xStar, xStar, sigmaF, l) - t(v)%*%v
xStar <- seq(-1,1,0.01)
sigmaF <- 1
l <- 0.3
sigmaNoise <- 0.1
x <- 0.4
y <- 0.719
K <-SquaredExpKernel(x, x, sigmaF, l)
L <- t(chol(K + sigmaNoise*diag(1, nrow(K),ncol(K))))
alpha <- solve(t(L), solve(L, y))
kStar <- SquaredExpKernel(x, xStar, sigmaF, l)
MeanPosterior <- t(kStar)%*%alpha # fstar normalisera?
v <- solve(L, kStar)
VarPosterior <- SquaredExpKernel(xStar, xStar, sigmaF, l) - t(v)%*%v
VarPosterior
install.packages("kernlab")
library("kernlab")
library(kernlab)
?rbfdot
install.packages("kernlab")
library(kernlab)
install.packages("kernlab")
library(kernlab)
SquaredExpKernel <- function(x1,x2,sigmaF=1,l=3){ #Implementera egen?
n1 <- length(x1)
n2 <- length(x2)
K <- matrix(NA,n1,n2)
for (i in 1:n2){
K[,i] <- sigmaF^2*exp(-0.5*( (x1-x2[i])/l)^2 )
}
return(K)
}
PlotLines <- function(x, y, grid, Mean, Variance){
UpperLimit <- Mean+1.96 * sqrt(Variance)
LowerLimit <- Mean-1.96 * sqrt(Variance)
matplot(grid, Mean, type='l', col ="red", xlab="x", ylab="y", ylim=c(-3, 3))
points(x, y, col="blue")
lines(grid, UpperLimit, type="l", col="green")
lines(grid, LowerLimit, type="l", col="green")
}
# xStar: grid
# x: datapunkter
# y: target
# hyperparam: vector with (sigmaf, l)
posteriorGP <- function(x, y, xStar, hyperParam, sigmaNoise){
sigmaF <- hyperParam[[1]]
l <- hyperParam[[2]]
K <-SquaredExpKernel(x, x, sigmaF, l)
L <- t(chol(K + sigmaNoise*diag(1, nrow(K),ncol(K))))
alpha <- solve(t(L), solve(L, y))
kStar <- SquaredExpKernel(x, xStar, sigmaF, l)
MeanPosterior <- t(kStar)%*%alpha
v <- solve(L, kStar)
VarPosterior <- SquaredExpKernel(xStar, xStar, sigmaF, l) - t(v)%*%v
return(list(MeanPosterior, VarPosterior))
}
xStar <- seq(-1,1,0.01)
x <- 0.4
y <- 0.719
posterior <- posteriorGP(x, y, xStar, c(1, 0.3), 0.1)
MeanPosterior <- posterior[[1]]
VarPosterior <- posterior[[2]]
PlotLines(x, y, xStar, MeanPosterior, diag(VarPosterior))
x <- c(-0.6, 0.4)
y <- c(-0.044, 0.719)
posterior <- posteriorGP(x, y, xStar, c(1, 0.3), 0.1)
MeanPosterior <- posterior[[1]]
VarPosterior <- posterior[[2]]
PlotLines(x, y, xStar, MeanPosterior, diag(VarPosterior))
x <- c(-1.0, -0.6, -0.2, 0.4, 0.8)
y <- c(0.768, -0.044, -0.940, 0.719, -0.664)
posterior <- posteriorGP(x, y, xStar, c(1, 0.3), 0.1)
MeanPosterior <- posterior[[1]]
VarPosterior <- posterior[[2]]
PlotLines(x, y, xStar, MeanPosterior, diag(VarPosterior))
posterior <- posteriorGP(x, y, xStar, c(1, 1), 0.1)
MeanPosterior <- posterior[[1]]
VarPosterior <- posterior[[2]]
PlotLines(x, y, xStar, MeanPosterior, diag(VarPosterior))
?gausspr
library(kernlab)
install.packages("AtmRay") # To make 2D grid like in Matlab's meshgrid
library(AtmRay)
setwd("~/Projects/aml/lab3")
setwd("~/Projects/aml/lab3")
############################################################################
###    Prelims: Setting path and installing/loading packages            #####
#############################################################################
#setwd('/Users/mv/Dropbox/Teaching/AdvML/GaussianProcess/Code')
setwd("~/Projects/aml/lab3")
#install.packages('kernlab')
#install.packages("AtmRay") # To make 2D grid like in Matlab's meshgrid
library(kernlab)
library(AtmRay)
X <- matrix(rnorm(12), 4, 3)
Xstar <- matrix(rnorm(15), 5, 3)
X
Xstar
?rnorm
??gausspr
?gausspr
?kernelMatrix
rbfdot
?rbfdot
SEkernel <- function(ell, sigmaf){
rval <- function(x, xtick){
return(sigmaf^2*exp(-(dist(x-xtick,method = "euclidean")))/(2*ell^2))
}
return(new("SEkernel", .Data = rval, kpar = list(ell, sigmaf)))
}
SEkernel(1, 3)
SEkernel <- function(ell, sigmaf){
rval <- function(x, xtick){
return(sigmaf^2*exp(-(dist(x-xtick,method = "euclidean")))/(2*ell^2))
}
return(new("SEkernel", .Data = rval, kpar = list(ell = ell, sigmaf = sigmaf)))
}
SEkernel(1, 3)
SEkernel(1, 3)
SEkernel <- function(ell, sigmaf){
rval <- function(x, xtick){
return(sigmaf^2*exp(-(dist(x-xtick,method = "euclidean")))/(2*ell^2))
}
return(new("SEkernel", .Data = rval, kpar = list(ell = ell, sigmaf = sigmaf)))
}
SEkernel(1, 3)
SEkernel <- function(sigmaf=1, ell=3){
rval <- function(x, xtick){
return(sigmaf^2*exp(-(dist(x-xtick,method = "euclidean")))/(2*ell^2))
}
return(new("SEkernel", .Data = rval, kpar = list(sigmaf = sigmaf,ell = ell)))
}
SEkernel(1, 3)
?rbfdot
rbfdot
SEkernel(1, 3)
ell = 1
sigmaf = 3
SEkernel <- function(sigmaf=1, ell=3){
rval <- function(x, xtick){
return(sigmaf^2*exp(-(dist(x-xtick,method = "euclidean")))/(2*ell^2))
}
return(rval)
}
SEkernel(1, 3)
SEkernel <- function(sigmaf=1, ell=3){
rval <- function(x, xtick){
return(sigmaf^2*exp(-(dist(x-xtick,method = "euclidean")))/(2*ell^2))
}
return(rval)
}
SEkernel(1, 3)
kernel <- SEkernel(sigmaf=1, ell=3)
kernel(3,4)
kernel(c(1,2), c(3,4))
kernel <- SEkernel(sigmaf=1, ell=1)
kernel(1,2)
?kernelMatrix
K <- mySE(c(1,3,4), c(2,3,4))
mySE <- SEkernel(sigmaf=1, ell=1)
mySE(1,2)
K <- mySE(c(1,3,4), c(2,3,4))
K
SEkernel <- function(sigmaf=1, ell=3){
rval <- function(x, xtick){
return(sigmaf^2*exp(-(dist(x-xtick,method = "euclidean")))/(2*ell^2))
}
return(rval)
}
mySE <- SEkernel(sigmaf=1, ell=1)
mySE(1,2)
K <- mySE(c(1,3,4), c(2,3,4))
K
